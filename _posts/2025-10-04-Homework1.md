---
title: Homework1
date: 2025-10-06 12:30:00 
categories: [homework, statistics]
tags: [homework]     # TAG names should always be lowercase
math: true
---

# Introduction to Statistics

Statistics is the discipline that concerns the collection, organization, analysis, interpretation  of data in order to extract knowledge, make decision and draw conclusions in presence of uncertainty. In a world increasingly driven by data, a foundational understanding of statistics is more crucial then ever. it's the key to unlocking insights, making informed decision, and navigating the complexities of modern lives.

## What is statistics?

At its core, statistics is about making sense of data, transforming raw numbers into meaningful information. It's a field that provides tools and methods to:

- collect data: This can be done through surveys, experiments or observation
- Analyze data: This involves using mathematical and computational techniques to identify patterns and relationships
- Interpret data: This is the process of drawing meaningful conclusions from the analysis.
- Present Data: This involves communicating findings in a clear and concise way, often using graphs and charts

## Basic concepts of statistics

### Population and samples

In statistics, a **population** is the *entire* group that you want to draw a conclusion about. It's every single member of a group of interest.

- example: all network connections recorded by a company in one year; all students in a university; all login attempts on a server.
- The population can be **finite** (limited number of observations) or **infinite** (theoretically, unbounded)
  
A **sample** is a **subset** of the population that is actually observed or measured.
Studying the whole population is often impossible or too expensive; researchers use a sample to estimate the properties of a population.

- example: analyzing 1000 network connection out of millions to study attack frequency
- A **good** sample must be **representative**, meaning it accurately reflects the characteristics of the **entire** population.

Sampling methods:

- **Random sampling**: every element has the same chance to be chosen
- **Stratified sampling**: the population is divided into groups, and samples are taken from each group.
- **Systematic sampling**: choosing every n-th element after a random start.

### Variables

A **variable** is any characteristic that can be measured or categorized.
There are two subgroups of variables: **Qualitative** and **Numerical**

#### Qualitative

These variables describe a quality or characteristic and are sorted into categories. They answer questions like "what kind?" or "which type?". You can't typically perform meaningful arithmetic with them (e.g., adding 'blue' + 'green' doesn't make sense).

- **Nominal**: These are categories with no intrinsic order or ranking. One is not "more" or "less" than another.

  - **Examples**: Eye color (Blue, Brown, Green), Country of birth, Type of car.

- **Ordinal**: These are categories that have a natural, logical order or rank, but the distance between the categories is not uniform or measurable.

  - **Examples**: Customer satisfaction ratings (Poor, Average, Good, Excellent), Education level (High School, Bachelor's, Master's), Military ranks.

#### Numerical

These variables represent measurable quantities and are expressed as numbers. They answer questions like "how much?" or "how many?". You can perform mathematical operations on them.

- **Discrete**: These variables can only take on specific, countable values (usually whole numbers). There are gaps between the possible values.

  - **Examples**: The number of students in a class (you can't have 25.5 students), the number of cars in a parking lot, the result of a dice roll.

- **Continuous**: These variables can take on any value within a given range. They are measured, not counted, and can be broken down into finer and finer units.

  - **Examples**: A person's height (1.75 meters, 1.751 meters, etc.), the temperature of a room, the weight of a package.

### Measure of Central Tendency

Those describe the ''center'' or **typical** values of a dataset.

#### Mean (Average)

The mean is the arithmetic average of all observations.

$\overline{x} = \sum_{i=1}^{n}x_i/n$

it is useful for symmetric distributions but can heavily influenced by outliers.

#### Median

The median is the middle value when **all** observations are **ordered**

- if the dataset has an odd number of elements: median = middle value
- if even: median = average of the two central values.

#### Mode

The **mode** is the value that appears more frequently

- a dataset may have one mode (unimodal) more then one (multimodal) or none.

### Measure of Dispersion

These describe how **spread** **out** the data is around the center.

#### Range

The range is the difference between the maximum and minimum value:

$Range=x_{max}-x_{min}$

It's very sensitive to outliers.

#### Variance

The variance measures the average sqared deviation from the mean:

$s^2={\sum_{i=1}^n({x_i-\overline{x}}^2})/({n-1})$

It quantifies how much the data points differ from the average value.

#### Standar deviation

The standard deviation is the square root of the variance:

$s=\sqrt{s^2}$

It expresses dispersion in the same unit as the original data, making it easier to interpret.

### Distribution of Data

A **distribution** show how values of a variable are spread across the possible outcomes.

#### Normal Distribution

The most common distribution is the **normal (Gaussian) distribution**, which is symmetric and bell-shaped.
It’s characterized by:

- The **mean (μ)** → center of the curve
- The **standard deviation (σ)** → width of the curve
In a normal distribution:
- ~68% of values fall within 1σ from the mean
- ~95% within 2σ
- ~99.7% within 3σ
This property (the “68–95–99.7 rule”) is widely used in anomaly detection, quality control, and performance analysis.

### Outliers

An outlier is an observation that lies far away from the majority of the data.
It may represent:

- Measurement or recording error
- A genuine but rare event
Detecting outliers is important because they can distort averages and mislead statistical inference, but in cybersecurity, they may represent the most valuable information (indicators of an attack).

# The role of statistics in cybersecurity

The core challenge of cybersecurity is to identify malicious or anomalous behavior within a sea of normal activity. Statistics provides the mathematical framework to do exactly that, transforming raw security data into actionable intelligence.

## Why statistics is important is cybersecurity

Cybersecurity data is inherently noisy, vast, and complex. Billions of events—like network connections, file accesses, and login attempts—are generated daily. Relying on predefined rules alone is insufficient because attackers constantly change their methods. Statistics helps us overcome this challenge in several key ways:

- **Establishing Baselines**: Before you can spot something abnormal, you must first understand what is normal. Statistics allow us to create a **baseline** of typical behavior for a network, a user, or an application. This baseline, defined by measures like *mean, median, and standard deviation,* becomes the reference point for detecting deviations.
- **Anomaly Detection**: This is arguably the most critical use of statistics in cybersecurity. Anomalies are data points that deviate significantly from the established baseline; in other words, they are **outliers**. By modeling normal behavior, statistical methods can automatically flag suspicious events that could signal an attack, such as an unusual number of login failures or a sudden spike in data transfer.
- **Reducing False Positives**: Security alert systems can generate thousands of warnings a day, many of which are false alarms. Statistical analysis helps tune these systems by distinguishing between truly anomalous events and benign statistical fluctuations, allowing security teams to focus on genuine threats.
- **Predictive Analysis and Risk Management**: By analyzing historical data on past incidents, statistics can be used to model the likelihood of future attacks. This helps organizations quantify risk, prioritize security investments, and predict emerging threats before they materialize.

## How is statistics used in practice?

Here are some concrete examples:

### Intrusion Detection and Network Monitoring

Network traffic can be modeled using statistical distributions:

- The **volume of data** transferred per hour, the **number of connections** to a specific port, or the **frequency of certain protocols** can be monitored.
- A sudden and extreme deviation from the normal distribution (a value falling more than 3 standard deviations from the mean) could indicate a **Denial-of-Service (DoS) attack**, a **port scan**, or **data exfiltration**.
- **Time-series analysis** can be used to model traffic patterns over time, helping to detect subtle, low-and-slow attacks that might otherwise go unnoticed

### Malware and Threat Signature analysis

Malware usually exhibits specific patterns. Statistics can help us identify these signatures:

- **Frequency analysis** of system calls, API requests, or byte sequences in a file can help distinguish malicious software from a benign program.

### UEBA (User and Entity Behavior Analytics)

The UEBA system builds a statistical profile for each user or device over a network. This profile includes:

- Typical login times and locations
- Commonly accessed files and servers
- Average data transfer volume

### Risk Assessment and Management

Statistical models, such as **Bayesian analysis**, can be used to calculate the probability of a security breach given certain vulnerabilities and threat intelligence. For example:

- An organization can calculate the financial risk of a potential data breach by modeling the probability of an attack succeeding and the expected financial loss if it does. This allows for data-driven decisions on where to allocate security resources.

# Conclusion

In summary, the journey from foundational statistical principles to practical cybersecurity applications reveals a powerful synergy. Statistics provides more than just a set of mathematical tools; it offers a systematic way of thinking that is essential for navigating the modern threat landscape. By understanding concepts like central tendency, dispersion, and distributions, we can establish baselines of normal behavior. By mastering the art of identifying outliers, we can unmask the subtle footprints of an attacker.
The methods discussed demonstrate that statistics transform raw security data into a proactive defense mechanism. It allows cybersecurity professionals to move beyond simply reacting to alerts and instead begin to quantify risk, anticipate threats, and make data-driven decisions. As technology evolves and the volume of data grows, the role of statistics will only become more critical, solidifying its place as an indispensable cornerstone of modern cybersecurity.
