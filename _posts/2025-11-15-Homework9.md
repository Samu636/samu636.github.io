---
title: Homework9
date: 2025-11-15 00:10:00 
categories: [TOP_CATEGORY, SUB_CATEGORY]
tags: [homework]     # TAG names should always be lowercase
css: /assets/css/prova.css
math: true
---
## Interpretations of Probability

The main interpretations differ on what a probability value *represents*.

* **Classical Probability:** This is the oldest interpretation. It defines probability as the ratio of **favorable outcomes** to the total number of **equally likely outcomes**.
  * **Formula:**
        $$P(E) = \frac{\text{Count of favorable outcomes}}{\text{Total count of possible outcomes}}$$
  * **Example:** The probability of rolling a 3 on a fair six-sided die is $1/6$, as there is one favorable outcome (rolling a 3) and six equally likely possible outcomes.
  * **Limitation:** It strictly requires a finite number of outcomes that are, by assumption, "equally likely," which isn't always applicable (e.g., a biased coin).

* **Frequentist (or Empirical) Probability:** This interpretation defines probability as the **long-run relative frequency** of an event over a large number of trials.
  * **Formula:**
        $$P(E) = \lim_{n \to \infty} \frac{\text{Number of times } E \text{ occurred}}{n \text{ (total trials)}}$$
  * **Example:** To find the probability of a biased coin landing heads, you would flip it thousands of times. If it lands heads 6,000 times in 10,000 flips, the frequentist would state the probability is approximately 0.6.
  * **Limitation:** It struggles with one-off events where repetition is impossible (e.g., "What is the probability that *this specific candidate* will win *this* election?").

* **Bayesian (or Subjective) Probability:** This interpretation treats probability as a **degree of belief** or confidence in a proposition, based on available evidence.
  * **Core Idea:** It uses **Bayes' Theorem** to update a "prior" belief with new evidence to form a "posterior" belief.
  * **Example:** An investor might assign a subjective 70% probability (a prior) to a stock increasing in value. After positive earnings are released (new evidence), they update this belief to, say, 85% (a posterior).
  * **Limitation:** It is inherently subjective, as different individuals can have different, yet valid, priors.

* **Geometric Probability:** This extends the classical definition to **uncountable (infinite) sample spaces**. It defines probability as the ratio of *measures* (like length, area, or volume).
  * **Formula:**
        $$P(E) = \frac{\text{Measure (length, area, etc.) of } E}{\text{Measure of the total sample space}}$$
  * **Example:** If you drop a pin on a 1-meter line, the probability it lands in the first 10 cm is $\frac{10 \text{ cm}}{100 \text{ cm}} = 0.1$.

---

## The Axiomatic Resolution

The various interpretations are inconsistent. They disagree on whether probability is a physical property of the world (frequentist), a logical ratio (classical), or a state of knowledge (Bayesian).

The **axiomatic approach**, formalized by **Andrey Kolmogorov** in 1933, resolves this by *ignoring* the philosophical debate. It doesn't define what probability *is*; it defines the mathematical **rules it must follow**.

It treats probability as an abstract mathematical object—a **measure**—that satisfies three simple rules (the axioms). This provides a unified mathematical language. A Bayesian and a frequentist may disagree on how they *arrived* at $P(A) = 0.4$, but they will both agree on how to *use* that number in calculations (e.g., $P(A^c) = 0.6$).

The axioms separate the **mathematical calculus** of probability from its **philosophical interpretation**, allowing everyone to use the same powerful toolbox.

---

## Relationship with Measure Theory

Probability theory is, formally, a branch of **measure theory**. The axiomatic approach is built directly on this foundation.

A **probability space** is defined as a **measure space** where the total measure is 1. This space is a triplet $(\Omega, \mathcal{F}, P)$.

* **1. Sample Space ($\Omega$):** This is the set of all possible outcomes of an experiment.
  * *Example (Die Roll):* $\Omega = \{1, 2, 3, 4, 5, 6\}$

* **2. Sigma-Algebra ($\sigma$-algebra, $\mathcal{F}$):** This is a collection of subsets of $\Omega$, called **events**. It represents all the outcomes we can meaningfully assign a probability to. It must satisfy three properties:
    1. It contains the whole sample space: $\Omega \in \mathcal{F}$.
    2. It is **closed under complement:** If an event $A$ is in $\mathcal{F}$, then its complement $A^c$ (not $A$) is also in $\mathcal{F}$.
    3. It is **closed under countable unions:** If $A_1, A_2, ...$ are all in $\mathcal{F}$, their union $\bigcup_{i=1}^\infty A_i$ is also in $\mathcal{F}$.

* **3. Probability Measure ($P$):** This is a function that maps events in $\mathcal{F}$ to a real number between 0 and 1. This function *is* the measure, and it must satisfy the **Kolmogorov Axioms**:
    1. **Non-negativity:** $P(A) \ge 0$ for all $A \in \mathcal{F}$.
    2. **Normalization:** $P(\Omega) = 1$ (The probability of *some* outcome happening is 1).
    3. **Countable Additivity:** For any countable collection of *disjoint* events $A_1, A_2, ...$ (meaning $A_i \cap A_j = \emptyset$ for $i \ne j$),
        $$P\left(\bigcup_{i=1}^\infty A_i\right) = \sum_{i=1}^\infty P(A_i)$$

**Random Variables**
A **random variable** is not random and is not a variable. It is a **measurable function** that maps outcomes from the sample space ($\Omega$) to the set of real numbers ($\mathbb{R}$).

* **Definition:** A function $X: \Omega \to \mathbb{R}$ is a random variable if it is **measurable** with respect to $\mathcal{F}$.
* **What "measurable" means:** It simply means that for any real number $x$, the set of outcomes $\{\omega \in \Omega \mid X(\omega) \le x\}$ is an event in our $\sigma$-algebra $\mathcal{F}$. This is a technical requirement that ensures we can ask questions like "What is the probability that $X$ is less than 5?" and get a valid answer.

---

## Derivations from the Axioms

We can derive all other properties of probability from the three axioms.

**Basis for Derivations (The Axioms):**

1. $P(A) \ge 0$
2. $P(\Omega) = 1$
3. If $A_1, A_2, ...$ are disjoint, $P(\bigcup A_i) = \sum P(A_i)$

*(We also use two simple properties derived directly from these: $P(\emptyset) = 0$ and $P(A^c) = 1 - P(A)$.)*

### 1. Subadditivity (Boole's Inequality)

**Property to Prove:** For *any* events $A$ and $B$ (not necessarily disjoint), $P(A \cup B) \le P(A) + P(B)$.

**Proof:**

1. We can express the union $A \cup B$ as the union of two **disjoint** sets: $A$ and $(B \cap A^c)$. (This is "all of A" and "the part of B that is not in A").
    * $A \cup B = A \cup (B \cap A^c)$
2. Since these two sets are disjoint, we can apply **Axiom 3**:
    * $P(A \cup B) = P(A) + P(B \cap A^c)$
3. Now, consider the event $B$. We can also decompose it into two disjoint sets: $(B \cap A)$ and $(B \cap A^c)$.
    * $B = (B \cap A) \cup (B \cap A^c)$
4. By **Axiom 3**:
    * $P(B) = P(B \cap A) + P(B \cap A^c)$
5. By **Axiom 1**, we know that $P(B \cap A) \ge 0$.
6. Therefore, from step 4, we must have $P(B) \ge P(B \cap A^c)$.
7. Now substitute this inequality back into the equation from step 2:
    * $P(A \cup B) = P(A) + P(B \cap A^c) \le P(A) + P(B)$
8. **Q.E.D.** $P(A \cup B) \le P(A) + P(B)$

[Image of Venn diagram for subadditivity]

### 2. Inclusion-Exclusion Principle (for two sets)

**Property to Prove:** $P(A \cup B) = P(A) + P(B) - P(A \cap B)$.

**Proof:**

1. We start with the same disjoint decomposition from step 1 of the subadditivity proof:
    * $A \cup B = A \cup (B \cap A^c)$
2. By **Axiom 3**, because $A$ and $(B \cap A^c)$ are disjoint:
    * $P(A \cup B) = P(A) + P(B \cap A^c)$  *(Equation 1)*
3. We also use the disjoint decomposition of $B$ from step 3 of the previous proof:
    * $B = (B \cap A) \cup (B \cap A^c)$
4. By **Axiom 3**:
    * $P(B) = P(B \cap A) + P(B \cap A^c)$
5. This time, we rearrange this equation to solve for $P(B \cap A^c)$:
    * $P(B \cap A^c) = P(B) - P(B \cap A)$
6. Finally, substitute this expression back into *Equation 1*:
    * $P(A \cup B) = P(A) + [P(B) - P(B \cap A)]$
7. **Q.E.D.** $P(A \cup B) = P(A) + P(B) - P(A \cap B)$

---

# Sources
<https://www.upgrad.com/blog/subjective-probability/>  
<https://www.ebsco.com/research-starters/mathematics/history-probability>
<https://en.wikipedia.org/wiki/Frequentist_probability>
<https://www.gauthmath.com/solution/1817551329295671/5-5-State-the-frequentist-interpretation-of-probability->
<https://en.wikipedia.org/wiki/Bayes%27_theorem>
<https://en.wikipedia.org/wiki/Probability_interpretations>
<https://en.wikipedia.org/wiki/Probability_space>
<https://www.stat.berkeley.edu/~wfithian/courses/stat210a/measure-theory-basics.html>